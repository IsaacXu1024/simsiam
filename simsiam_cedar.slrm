#!/bin/bash
#SBATCH --time=00:30:00             # max walltime, hh:mm:ss
#SBATCH --nodes 1                   # Number of nodes to request
#SBATCH --gres=gpu:2                # Number of GPUs per node to request
#SBATCH --tasks-per-node=1          # Number of processes to spawn per node
#SBATCH --cpus-per-task=12          # Number of CPUs per GPU
#SBATCH --mem=64G                   # Memory per node
#SBATCH --output=logs/%x_%A-%a_%n-%t.out
                                    # %x=job-name, %A=job ID, %a=array value, %n=node rank, %t=task rank, %N=hostname
                                    # Note: You must manually create output directory "logs" before launching job.
#SBATCH --job-name=simsiam
#SBATCH --account=def-ttt   # Use default account

# Manually define this variable to be equal to the number of GPUs in the --gres argument above
GPUS_PER_NODE=2

# sbatch for Graham/Cedar
# Based on https://docs.computecanada.ca/wiki/PyTorch/en
# and https://github.com/facebookresearch/simsiam/blob/main/README.md

date
echo ""
echo "Job $SLURM_JOB_NAME ($SLURM_JOB_ID) begins on $SLURM_NODENAME, submitted from $SLURM_SUBMIT_HOST ($SLURM_CLUSTER_NAME)"
echo ""
echo "Main script will run on host tcp://localhost:10001 with backend nccl"
echo ""
echo "SLURM_CLUSTER_NAME      = $SLURM_CLUSTER_NAME"        # Name of the cluster on which the job is executing.
echo "SLURM_NODENAME          = $SLURM_NODENAME"
echo "SLURM_JOB_QOS           = $SLURM_JOB_QOS"             # Quality Of Service (QOS) of the job allocation.
echo "SLURM_JOB_ID            = $SLURM_JOB_ID"              # The ID of the job allocation.
echo "SLURM_RESTART_COUNT     = $SLURM_RESTART_COUNT"       # The number of times the job has been restarted.
if [ "$SLURM_ARRAY_TASK_COUNT" != "" ] && [ "$SLURM_ARRAY_TASK_COUNT" -gt 1 ]; then
    echo ""
    echo "SLURM_ARRAY_JOB_ID      = $SLURM_ARRAY_JOB_ID"        # Job array's master job ID number.
    echo "SLURM_ARRAY_TASK_COUNT  = $SLURM_ARRAY_TASK_COUNT"    # Total number of tasks in a job array.
    echo "SLURM_ARRAY_TASK_ID     = $SLURM_ARRAY_TASK_ID"       # Job array ID (index) number.
    echo "SLURM_ARRAY_TASK_MAX    = $SLURM_ARRAY_TASK_MAX"      # Job array's maximum ID (index) number.
    echo "SLURM_ARRAY_TASK_STEP   = $SLURM_ARRAY_TASK_STEP"     # Job array's index step size.
fi;
echo ""
echo "SLURM_JOB_NUM_NODES     = $SLURM_JOB_NUM_NODES"       # Total number of nodes in the job's resource allocation.
echo "SLURM_JOB_NODELIST      = $SLURM_JOB_NODELIST"        # List of nodes allocated to the job.
echo "SLURM_TASKS_PER_NODE    = $SLURM_TASKS_PER_NODE"      # Number of tasks to be initiated on each node.
echo "SLURM_NTASKS            = $SLURM_NTASKS"              # Number of tasks to spawn.
echo "SLURM_PROCID            = $SLURM_PROCID"              # The MPI rank (or relative process ID) of the current process
echo ""
echo "SBATCH_GRES             = $SBATCH_GRES"               # Same as --gres
echo "GPUS_PER_NODE           = $GPUS_PER_NODE"             # Number of GPUs requested. Only set if the -G, --gpus option is specified.
echo "SLURM_CPUS_ON_NODE      = $SLURM_CPUS_ON_NODE"        # Number of CPUs allocated to the batch step.
echo "SLURM_JOB_CPUS_PER_NODE = $SLURM_JOB_CPUS_PER_NODE"   # Count of CPUs available to the job on the nodes in the allocation.
echo "SLURM_CPUS_PER_TASK     = $SLURM_CPUS_PER_TASK"       # Number of cpus requested per task. Only set if the --cpus-per-task option is specified.
echo ""
echo "------------------------------------------------------------------------"
echo ""
# Input handling
DATASET="$1"
echo "DATASET = $DATASET"
echo "EXTRA_ARGS = ${@:2}"
echo ""
echo "------------------------------------------------------------------------"
echo ""
pwd
echo ""
echo "commit ref:"
git rev-parse HEAD
echo ""
git status
echo ""
echo "------------------------------------------------------------------------"
echo ""
date
echo ""
echo "# Starting environment set up"

# Load CC modules
echo ""
echo "Loading modules"
echo ""
module load python/3.8
module load cuda cudnn
module load scipy-stack

# Make an environment, housed on the node's local SSD storage
ENV_DIR="$SLURM_TMPDIR/env"
echo ""
echo "Creating environment $ENV_DIR"
echo ""
virtualenv --no-download "$ENV_DIR"
source "$ENV_DIR/bin/activate"

# Install pytorch
echo ""
echo "Installing packages into $ENV_DIR"
echo ""
# The recommend way is just to do this, but then it is for the wrong CUDA version
# python -m pip install --no-index torch torchvision

# Work out what version of CUDA we are using
CUDA_VERSION=$(nvcc --version | sed -n 's/^.*release \([0-9\.]\+\).*$/\1/p');
MOD="+cu${CUDA_VERSION/./}";
echo "Installing pytorch with $MOD for CUDA $CUDA_VERSION"
python -m pip install \
    "torch==1.9.0${MOD}" \
    "torchvision==0.10.0${MOD}" \
    -f https://download.pytorch.org/whl/torch_stable.html

python -m pip install -r requirements.txt

echo "------------------------------------------------------------------------"
echo ""
date
echo ""
echo "# Debugging outputs"
echo ""
echo "pwd:"
pwd
echo ""
echo "python --version"
python --version
echo ""
echo "which pip:"
which pip
echo ""
echo "pip freeze:"
echo ""
pip freeze
echo ""
echo "which nvcc:"
which nvcc
echo ""
echo "nvcc --version"
nvcc --version
echo ""
echo "nvidia-smi"
nvidia-smi
echo ""
python -c "import torch; print('pytorch={}, cuda={}, gpus={}'.format(torch.__version__, torch.cuda.is_available(), torch.cuda.device_count()))"
echo ""
python -c "import torch; print(str(torch.zeros(2, device=torch.device('cuda')))); print('able to use cuda')"
echo ""
echo "------------------------------------------------------------------------"
echo "# Handling data on the node"
echo ""
echo "df -h:"
df -h
echo ""

echo "ls -lh ${SLURM_TMPDIR}:"
ls -lh "${SLURM_TMPDIR}"
echo ""

DATA_DIR="${SLURM_TMPDIR}/datasets"
rm -rf "$DATA_DIR"
mkdir -p "$DATA_DIR"

echo "ls -lh ${DATA_DIR}:"
ls -lh "${DATA_DIR}"
echo ""

CKPT_DIR="${SLURM_TMPDIR}/checkpoint"
mkdir -p "$CKPT_DIR"

echo "ls -lh ${CKPT_DIR}:"
ls -lh "${CKPT_DIR}"
echo ""

# Specify where to place checkpoints for long term storage once the job is
# finished
OUTPUT_DIR="$PWD/checkpoints"
JOB_OUTPUT_DIR="$OUTPUT_DIR/${SLURM_JOB_NAME}_${SLURM_JOB_ID}"

# -------------------------------------------------------------------------
echo ""
date
echo ""

if [ "$DATASET" == "imagenet" ]; then
    DATA_DIR="$DATA_DIR/$DATASET"
    echo "# Copying imagenet dataset to local node's SSD storage, ${DATA_DIR}"

    rsync -vz /project/rrg-ttt/datasets/imagenet/* "${DATA_DIR}/"

    mkdir -p "$DATA_DIR/train"
    mkdir -p "$DATA_DIR/val"

    echo "Extracting training data"
    tar -C "${DATA_DIR}/train" -xf "${DATA_DIR}/ILSVRC2012_img_train.tar"
    find "${DATA_DIR}/train" -name "*.tar" | while read NAME ; do mkdir -p "${NAME%.tar}"; tar -xf "${NAME}" -C "${NAME%.tar}"; rm -f "${NAME}"; done
    echo ""

    echo "Extracting validation data"
    tar -C "${DATA_DIR}/val" -xf "${DATA_DIR}/ILSVRC2012_img_val.tar"

    # Move validation images to subfolders:
    VAL_PREPPER="$DATA_DIR/valprep.sh"
    if test -f "$VAL_PREPPER"; then
        echo "Downloading valprep.sh";
        wget https://raw.githubusercontent.com/soumith/imagenetloader.torch/b81dbb1/valprep.sh -O "$VAL_PREPPER";
    fi
    echo "Moving validation data into subfolders"
    (cd "${DATA_DIR}/val"; sh "$VAL_PREPPER")
    echo ""

    # Check total files after extract
    #
    #  find train/ -name "*.JPEG" | wc -l
    #  # Should be 1281167
    #  find val/ -name "*.JPEG" | wc -l
    #  # Should be 50000

elif [ "$DATASET" == "imagenette2-160" ]; then
    DATA_DIR="$DATA_DIR/$DATASET"
    echo "# Copying imagenette2-160 dataset to local node's SSD storage, ${DATA_DIR}"

    rsync -zv "/project/rrg-ttt/datasets/imagenette2/imagenette2-160.tgz" "${DATA_DIR}/"

    echo ""
    echo "Extracting data"
    tar -C "${DATA_DIR}" -xf "${DATA_DIR}/imagenette2-160.tgz"
    DATA_DIR="$DATA_DIR/imagenette2-160"

elif [ -d "$DATASET" ]; then
    echo ""
    echo "# Copying directory $DATASET to local node's SSD storage, ${DATA_DIR}"
    rsync -zv "$DATASET" "${DATA_DIR}/"

elif [ -f "$DATASET" ]; then
    echo ""

    DATASET_FNAME="${DATASET##*/}"
    DATASET_NAME="${DATASET_FNAME%%.*}"
    DATASET_EXT="${DATASET_FNAME#*.}"
    TARGET_FNAME="${DATA_DIR}/${DATASET_FNAME}"

    if [ "$DATASET_EXT" == "tar.gz" ] || [ "$DATASET_EXT" == "tgz" ] || [ "$DATASET_EXT" == "tar" ]; then
        echo ""
    else
        echo "Unsupported file extension: $DATASET_EXT"
        exit;
    fi
    echo "# Copying file $DATASET to local node's SSD storage, ${DATA_DIR}"
    rsync -zv "$DATASET" "${DATA_DIR}/"

    echo "Untarring file $TARGET_FNAME"
    tar -C "${DATA_DIR}" -xf "$TARGET_FNAME"
    DATA_DIR="$DATA_DIR/$DATASET_NAME"

else
    echo "Invalid dataset name: $DATASET"
    exit;

fi

echo ""
echo "------------------------------------------------------------------------"
echo ""
date
echo ""

export NCCL_BLOCKING_WAIT=1     # Set this if you want to use the NCCL backend for inter-GPU communication.
export MASTER_ADDR=$(hostname)  # Store the master nodeâ€™s IP address in the MASTER_ADDR environment variable.

echo "r$SLURM_NODEID master: $MASTER_ADDR"
echo "r$SLURM_NODEID Launching python script"

# Launch your script
echo ""
echo "# Main script begins with host tcp://localhost:10001 with backend nccl"
echo ""

# Unsupervised Pre-Training
echo "Unsupervised Pre-Training"
#
# Both the batch size and number of workers get divided by ngpus_per_node in the main_simsiam.py script.
# Batch size parameter is the total batch size across all GPUs on the current node
# We want the batch size to be as large we can, which is about 48 per GPU.
#
BATCH_SIZE="$(( 48 * SLURM_JOB_NUM_NODES * GPUS_PER_NODE ))"
N_WORKERS="$SLURM_JOB_CPUS_PER_NODE"
#
echo "BATCH_SIZE = $BATCH_SIZE"
echo "N_WORKERS  = $N_WORKERS"
#
# We just need to call the main_simsiam script once and it will launch all processes on each node, for each GPU, for us automatically.
#
# To do unsupervised pre-training of a ResNet-50 model on ImageNet, run:
python main_simsiam.py \
    -a resnet18 \
    --dist-url 'tcp://localhost:10001' \
    --multiprocessing-distributed \
    --world-size "$SLURM_JOB_NUM_NODES" \
    --rank 0 \
	--epochs 5 \
    --fix-pred-lr \
    --workers "$N_WORKERS" \
    --batch-size "$BATCH_SIZE" \
    --checkpoint-dir "$CKPT_DIR" \
    --resume "$CKPT_DIR/checkpoint_latest.pt" \
    "$DATA_DIR" \
    "${@:2}"

echo ""
CKPT_SOURCE=$(readlink "$CKPT_DIR/checkpoint_latest.pt")
echo "Copying output checkpoint $CKPT_DIR to $JOB_OUTPUT_DIR"
mkdir -p "$JOB_OUTPUT_DIR"
rsync -rvz "$CKPT_SOURCE" "$JOB_OUTPUT_DIR"
ln -sfn "${CKPT_SOURCE##*/}" "$JOB_OUTPUT_DIR/checkpoint_latest.pt"

echo ""
echo "------------------------------------------------------------------------"
echo ""
date
echo ""

# Linear Classification
echo "Training linear classifier"
# With a pre-trained model, to train a supervised linear classifier on frozen features/weights
python main_lincls.py \
    -a resnet18 \
    --dist-url 'tcp://localhost:10001' \
    --multiprocessing-distributed \
    --world-size "$SLURM_JOB_NUM_NODES" \
    --rank 0 \
	--epochs 5 \
    --pretrained "$CKPT_DIR/checkpoint_latest.pt" \
    --lars \
    --workers "$N_WORKERS" \
    --batch-size "$BATCH_SIZE" \
    --checkpoint-dir "$CKPT_DIR" \
    --resume "$CKPT_DIR/lincls_checkpoint_latest.pt" \
    "$DATA_DIR"

echo ""
CKPT_SOURCE=$(readlink "$CKPT_DIR/lincls_checkpoint_latest.pt")
echo "Copying output checkpoint $CKPT_DIR to $JOB_OUTPUT_DIR"
mkdir -p "$JOB_OUTPUT_DIR"
rsync -rvz "$CKPT_SOURCE" "$JOB_OUTPUT_DIR"
ln -sfn "${CKPT_SOURCE##*/}" "$JOB_OUTPUT_DIR/lincls_checkpoint_latest.pt"
